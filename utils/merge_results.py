# VBench + Eval

import os
import json
import re
import shutil
import argparse
from glob import glob
import pandas as pd


def list_subfolders(folder_path):
    subfolders = [name for name in os.listdir(folder_path)
                  if os.path.isdir(os.path.join(folder_path, name))]
    return subfolders

def parse_score_file(filepath, pattern):
    scores = {}
    with open(filepath, "r", encoding="utf-8") as f:
        for line in f:
            match = re.search(pattern, line)
            if match:
                filename = match.group(1)
                try:
                    score = float(match.group(2))
                    scores[filename] = score
                except ValueError:
                    continue
    return scores

def merge_benchmarks(folder):
    # Step 1: Load benchmark JSON
    json_files = glob(os.path.join(folder, "eval_*.json"))
    if not json_files:
        print(f"âŒ {folder} - benchmark JSON íŒŒì¼ ì—†ìŒ")
        return
    with open(json_files[0], "r", encoding="utf-8") as f:
        benchmark_data = json.load(f)

    # Step 1.1: dynamic_degree ê°’ì„ True/Falseì—ì„œ 0.9/0.1ìœ¼ë¡œ ë³€ê²½
    if "dynamic_degree" in benchmark_data:
        if isinstance(benchmark_data["dynamic_degree"], list) and len(benchmark_data["dynamic_degree"]) > 1:
            for entry in benchmark_data["dynamic_degree"][1]:
                if "video_results" in entry and isinstance(entry["video_results"], bool):
                    entry["video_results"] = 0.9 if entry["video_results"] else 0.1

    # Step 2: ëª¨ë“  metric íŒŒì¼ ì •ì˜
    metric_files = {
        "inception_score": ("IS_record.txt", r".*/(\d+\.mp4):\s+([\d\.eE\+\-]+)"),
        "face_consistency_score": ("face_consistency_score_record.txt", r"Vid: (\d+\.mp4),\s+Current face_consistency_score: ([\d\.]+)"),
        "sd_score": ("sd_score_record.txt", r"Vid: (\d+\.mp4),\s+Current sd_score: ([\d\.]+)"),
        "flow_score": ("flow_score_record.txt", r"Vid: (\d+\.mp4),\s+Current flow_score: ([\d\.eE\+\-]+)"),
        "warping_error": ("warping_error_record.txt", r"Vid: (\d+\.mp4),\s+Current warping_error: ([\d\.eE\-]+)"),
        "blip_bleu": ("blip_bleu_record.txt", r"Vid: (\d+\.mp4),\s+Current blip_bleu: ([\d\.eE\+\-]+)"),
        "clip_score": ("clip_score_record.txt", r"Vid: (\d+\.mp4),\s+Current clip_score: ([\d\.eE\+\-]+)"),
        "clip_temp_score": ("clip_temp_score_record.txt", r"Vid: (\d+\.mp4),\s+Current clip_temp_score: ([\d\.eE\+\-]+)")
    }

    # Step 3: ê° metric íŒŒì¼ íŒŒì‹±
    all_scores = {}
    for metric, (filename, pattern) in metric_files.items():
        path = os.path.join(folder, filename)
        if os.path.exists(path):
            parsed = parse_score_file(path, pattern)
            for fname, score in parsed.items():
                all_scores.setdefault(fname, {})[metric] = score

    # Step 4: dover.csv (ë’¤ì—ì„œë¶€í„° 3ì—´ ê¸°ì¤€ + ì˜ˆì™¸ ì¤„ ë¬´ì‹œ)
    dover_path = os.path.join(folder, "dover.csv")
    if os.path.exists(dover_path):
        bad_lines = []
        clean_rows = []

        with open(dover_path, "r", encoding="utf-8") as f:
            header = f.readline()  # skip header
            for i, line in enumerate(f, start=2):  # 2ë¶€í„° ì‹œì‘ (ì¤„ ë²ˆí˜¸)
                line = line.strip()
                if not line or line.startswith("Avg."):
                    continue  # í‰ê·  ì¤„ì€ ê±´ë„ˆëœ€

                parts = line.rsplit(",", 3)
                if len(parts) < 4:
                    bad_lines.append((i, line))
                    continue

                path = parts[0].strip()
                aesthetic = parts[1].strip()
                technical = parts[2].strip()

                try:
                    filename = f"{path}.mp4"
                    all_scores.setdefault(filename, {})["aesthetic_score"] = float(aesthetic)
                    all_scores[filename]["technical_score"] = float(technical)
                except ValueError:
                    bad_lines.append((i, line))

        if bad_lines:
            print("âš ï¸ dover.csvì—ì„œ íŒŒì‹± ì‹¤íŒ¨í•œ ì¤„:")
            for lineno, content in bad_lines:
                print(f"  Line {lineno}: {content}")

    # Step 5: ëª¨ë“  metric ì´ë¦„ (benchmark + ì™¸ë¶€)
    benchmark_metrics = list(benchmark_data.keys())
    external_metrics = list(metric_files.keys()) + ["aesthetic_score", "technical_score"]
    required_metrics = benchmark_metrics + external_metrics

    # Step 6: ë³‘í•© (ëª¨ë“  metric ë‹¤ ìˆì„ ë•Œë§Œ í¬í•¨)
    merged = {}

    for metric, data in benchmark_data.items():
        if isinstance(data, list) and len(data) > 1:
            for entry in data[1]:
                path = entry.get("video_path")
                if not path:
                    continue
                filename = os.path.basename(path)
                if filename not in all_scores:
                    continue

                # ì„ì‹œ dict ìƒì„±
                temp = {metric: entry["video_results"]}
                for other_metric in benchmark_metrics:
                    if other_metric == metric:
                        continue
                    for other_entry in benchmark_data[other_metric][1]:
                        if os.path.basename(other_entry.get("video_path", "")) == filename:
                            temp[other_metric] = other_entry.get("video_results")
                            break

                # ì™¸ë¶€ metric ì¶”ê°€
                for m in external_metrics:
                    if m in all_scores[filename]:
                        temp[m] = all_scores[filename][m]

                # âœ… ëˆ„ë½ëœ metric ê²€ì‚¬
                missing = [k for k in required_metrics if k not in temp]
                if missing:
                    print(f"âš ï¸ {filename} ëˆ„ë½ëœ metric: {missing}")
                else:
                    merged[filename] = temp

    # Step 7: ì €ì¥
    output_path = os.path.join(folder, "merged_results.json")
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(merged, f, indent=4, ensure_ascii=False)

    print(f"âœ… ë³‘í•© ì™„ë£Œ: {output_path} | í¬í•¨ëœ ì˜ìƒ ìˆ˜: {len(merged)}")


def find_timestamp_folder(base_path, timestamp):
    """ì£¼ì–´ì§„ timestampê°€ í¬í•¨ëœ í´ë”ë¥¼ ì°¾ìŠµë‹ˆë‹¤."""
    for item in os.listdir(base_path):
        item_path = os.path.join(base_path, item)
        if os.path.isdir(item_path) and timestamp in item:
            return item_path
    return None


def find_timestamp_files(base_path, timestamp):
    """ì£¼ì–´ì§„ timestampê°€ í¬í•¨ëœ íŒŒì¼ ëª©ë¡ì„ ì°¾ìŠµë‹ˆë‹¤."""
    matching_files = []
    for item in os.listdir(base_path):
        item_path = os.path.join(base_path, item)
        if os.path.isfile(item_path) and timestamp in item:
            matching_files.append(item_path)
    return matching_files


def main():
    parser = argparse.ArgumentParser(description="MSGscore ë³‘í•© ë„êµ¬")
    parser.add_argument("--ec_path", required=True, help="MSGscore ê²½ë¡œ")
    parser.add_argument("--prompt_dir", help="í”„ë¡¬í”„íŠ¸ ë””ë ‰í† ë¦¬ ê²½ë¡œ")
    parser.add_argument("--output_dir", help="ì¶œë ¥ ë””ë ‰í† ë¦¬ ê²½ë¡œ")
    parser.add_argument("--timestamp", required=True, help="íƒ€ì„ìŠ¤íƒ¬í”„")
    
    args = parser.parse_args()
    
    # 1. MSGscore/EvalCrafter/resultì—ì„œ timestampì™€ ì¼ì¹˜í•˜ëŠ” í´ë” ì°¾ê¸°
    eval_path = os.path.join(args.ec_path, "EvalCrafter", "result")
    eval_folder = find_timestamp_folder(eval_path, args.timestamp)
    
    if not eval_folder:
        print(f"âŒ íƒ€ì„ìŠ¤íƒ¬í”„ {args.timestamp}ì™€ ì¼ì¹˜í•˜ëŠ” í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {eval_path}")
        return
    
    print(f"âœ… ì°¾ì€ EvalCrafter ê²°ê³¼ í´ë”: {eval_folder}")
    
    # 2. MSGscore/resultë¡œ í´ë” ë³µì‚¬
    result_path = os.path.join(args.ec_path, "result")
    os.makedirs(result_path, exist_ok=True)
    
    folder_name = os.path.basename(eval_folder)
    target_folder = os.path.join(result_path, folder_name)
    
    if os.path.exists(target_folder):
        print(f"âš ï¸ ì´ë¯¸ ì¡´ì¬í•˜ëŠ” í´ë”ì…ë‹ˆë‹¤: {target_folder}")
    else:
        shutil.copytree(eval_folder, target_folder)
        print(f"âœ… í´ë” ë³µì‚¬ ì™„ë£Œ: {target_folder}")
    
    # 3. MSGscore/Vench/resultì—ì„œ timestampì™€ ì¼ì¹˜í•˜ëŠ” íŒŒì¼ ì°¾ê¸°
    vbench_path = os.path.join(args.ec_path, "VBench", "result")
    vbench_files = find_timestamp_files(vbench_path, args.timestamp)
    
    if not vbench_files:
        print(f"âš ï¸ VBenchì—ì„œ íƒ€ì„ìŠ¤íƒ¬í”„ {args.timestamp}ì™€ ì¼ì¹˜í•˜ëŠ” íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    else:
        print(f"âœ… ì°¾ì€ VBench ê²°ê³¼ íŒŒì¼: {len(vbench_files)}ê°œ")
        
        # 4. íŒŒì¼ë“¤ì„ ë³µì‚¬í•œ í´ë”ì— ë³µì‚¬
        for file_path in vbench_files:
            file_name = os.path.basename(file_path)
            target_file = os.path.join(target_folder, file_name)
            shutil.copy2(file_path, target_file)
        
        print(f"âœ… VBench íŒŒì¼ ë³µì‚¬ ì™„ë£Œ: {target_folder}")
    
    # 5. ë³‘í•© ì‘ì—… ì‹¤í–‰
    print(f"ğŸ”„ ë³‘í•© ì‘ì—… ì‹œì‘: {target_folder}")
    merge_benchmarks(target_folder)


if __name__ == "__main__":
    main()

